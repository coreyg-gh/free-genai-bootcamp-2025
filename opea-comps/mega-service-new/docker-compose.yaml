networks:
  megaservice_network:
    name: megaservice_network
    driver: bridge
services:
  megaservice:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: megaservice
    ports:
      - "8888:8888"
    ipc: host
    restart: always
    networks:
      - megaservice_network
  ollama-server:
    image: ollama/ollama:latest
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
        limits:
          memory: 23G
    ports:
      - "9000:11434"
    hostname: 0.0.0.0
    volumes:
      - ollama_data:/root/.ollama
    environment:
      no_proxy: ${no_proxy}
      http_proxy: ${http_proxy}
      https_proxy: ${https_proxy}
      LLM_MODEL_ID: ${LLM_MODEL_ID}
      host_ip: ${host_ip}
    networks:
      - megaservice_network
  speecht5-service:
    image: ${REGISTRY:-opea}/speecht5:${TAG:-latest}
    container_name: speecht5-service
    ports:
      - ${SPEECHT5_PORT:-7055}:7055
    ipc: host
    user: root
    environment:
      no_proxy: ${no_proxy}
      http_proxy: ${http_proxy}
      https_proxy: ${https_proxy}
      HF_HUB_ENABLE_HF_TRANSFER: "1"
      HF_ENDPOINT: "https://huggingface.co"
      HF_HUB_CACHE: "/cache"
      CURL_CA_BUNDLE: ""
      REQUESTS_CA_BUNDLE: ""
    volumes:
      - type: volume
        source: hf_cache
        target: /cache
    entrypoint: []
    command: >
      bash -c "
      pip install --no-cache-dir hf_transfer &&
      mkdir -p /cache &&
      chmod 777 /cache &&
      python speecht5_server.py
      --host 0.0.0.0
      --port 7055
      --device cpu
      "
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7055/health"]
      interval: 300s
      timeout: 6s
      retries: 18
  #tts-speecht5:
  #  image: ${REGISTRY:-opea}/tts:${TAG:-latest}
  #  container_name: tts-speecht5-service
  #  ports:
  #    - ${TTS_PORT:-9088}:9088
  #  ipc: host
  #  environment:
  #    TTS_ENDPOINT: http://172.24.230.22:7055
  #    TTS_COMPONENT_NAME: ${TTS_COMPONENT_NAME:-OPEA_SPEECHT5_TTS}
  #  depends_on:
  #    speecht5-service:
  #      condition: service_healthy
  gptsovits-service:
    build:
      context: ./GPT-SoVITS
      dockerfile: Dockerfile
    container_name: gpt-sovits-service
    ports:
      - ${GPT_SOVITS_PORT:-9880}:9880
    ipc: host
    volumes:
      - ./audio:/audio
    environment:
      no_proxy: ${no_proxy}
      http_proxy: ${http_proxy}
      https_proxy: ${https_proxy}
      PYTHONPATH: "/usr/local/lib/python3.10/site-packages"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9880/health"]
      interval: 300s
      timeout: 6s
      retries: 18
  tts-gptsovits:
    image: ${REGISTRY:-opea}/tts:${TAG:-latest}
    container_name: tts-gptsovits-service
    ports:
      - ${TTS_PORT:-9088}:9088
    ipc: host
    environment:
      TTS_ENDPOINT: http://gptsovits-service:9880  # Using service name instead of IP
      TTS_COMPONENT_NAME: ${TTS_COMPONENT_NAME:-OPEA_GPTSOVITS_TTS}
    depends_on:
      gptsovits-service:
        condition: service_healthy
  #vllm-service:
  #  image: ${REGISTRY:-opea}/vllm:${TAG:-latest}
  #  container_name: vllm-service
  #  ports:
  #    - "9009:80"
  #  volumes:
  #    - "./data:/data"
  #  shm_size: 128g
  #  environment:
  #    no_proxy: ${no_proxy}
  #    http_proxy: ${http_proxy}
  #    https_proxy: ${https_proxy}
  #    HF_TOKEN: ${HUGGINGFACEHUB_API_TOKEN}
  #    LLM_MODEL_ID: ${LLM_MODEL_ID}
  #    VLLM_TORCH_PROFILER_DIR: "/mnt"
  #  deploy:
  #    resources:
  #      reservations:
  #        devices:
  #          - driver: nvidia
  #            count: 1
  #            capabilities: [gpu]
  #  healthcheck:
  #    test: ["CMD-SHELL", "curl -f http://$host_ip:9009/health || exit 1"]
  #    interval: 300s
  #    timeout: 10s
  #    retries: 100
  #  command: --model meta-llama/Llama-3.2-1B --host 0.0.0.0 --port 80
volumes:
  hf_cache:
  ollama_data:
